{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4k6OTX7Mmmo5e2FzGesIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadepanths/Python-PyTorch-Tutorial/blob/main/Tensors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMW11qS2FFCW"
      },
      "source": [
        "# Initializing a Tensor\n",
        "You can initialize a tensor in many ways. Here is the reference source, [TENSORS](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxOfpMUgMaqO"
      },
      "source": [
        "## Directly With Operator\n",
        "A tensor can be initialized directly using ```torch.tensor()``` to create a specific tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDvzLHyENv7i"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "t1 = torch.tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "t2 = torch.tensor([[5, 6],\n",
        "                  [7, 8]])\n",
        "\n",
        "print(\"t1: \\n\", t1)\n",
        "print(\"t2: \\n\", t2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNs-l1FpPfig"
      },
      "source": [
        "## Directly From Arrays\n",
        "Tensors can be initialized from a created array. The data type is automatically inferred."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjeMjF3PPukR"
      },
      "source": [
        "data = [[1, 2], [3, 4]]\n",
        "tensor_data = torch.tensor(data)\n",
        "print(f\"tensor_data from arrays: \\n {tensor_data} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUrpxs7UP6Vh"
      },
      "source": [
        "## From a NumPy Array\n",
        "Tensors can be created from NumPy arrays and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yBCmSlnQvNe"
      },
      "source": [
        "# Tensor from Numpy\n",
        "data = [[1, 2], [3, 4]]\n",
        "np_array = np.array(data)\n",
        "tensor_from_np = torch.from_numpy(np_array)\n",
        "print(f\"From Numpy: \\n {tensor_from_np} \\n\")\n",
        "\n",
        "# NumPy from Tensor\n",
        "np_from_tensor = np.array(tensor_from_np)\n",
        "print(f\"From Tensor: \\n {np_from_tensor} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyKNEnufRZgH"
      },
      "source": [
        "## From Another Tensor\n",
        "The newly created tensor retains the properties: shape and datatype of the argument tensor unless explicitly overridden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i65nC32kRlHc"
      },
      "source": [
        "tensor_ones = torch.ones_like(tensor_data)\n",
        "print(f\"Ones Tensor: \\n {tensor_ones} \\n\")\n",
        "\n",
        "tensor_rand = torch.rand_like(tensor_data, dtype=torch.float)\n",
        "print(f\"Random Tensor: \\n {tensor_rand} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9NiDSDAAR4W"
      },
      "source": [
        "## With a Random/Constant Values\n",
        "shape is a tuple of tensor dimensions. You can initialize a tensor with any constant value or random numbers. <br/>\n",
        "*rand* is random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcN38R-yBhB_"
      },
      "source": [
        "shape = (4, 3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRK7OefMBwkF"
      },
      "source": [
        "# Tensor's Attributes\n",
        "Attributes describe their shape, datatype, and the device on which the tensor is stored.\n",
        "\n",
        "\n",
        "\n",
        "*   *tensor.shape* will show the dimension of the tensor.\n",
        "*   *tensor.dtype* will show the datatype of the tensor.\n",
        "*   *tensor.device* will show the device the tensor is stored on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL5BmfY2CSYP"
      },
      "source": [
        "import torch\n",
        "\n",
        "tensor = torch.rand(3, 4,)\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGoAIHHYCZQi"
      },
      "source": [
        "#Tensor's Dimension/shape\n",
        "Since dimensions have been mentioned multiple times, here is some information regarding them. This section will help you visualizing multidimensions tensor/arrays. You can also read more about it here [Understanding Dimensions in PyTorch](https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuOk1wPnC2KR"
      },
      "source": [
        "## Rank-0 or Scalar\n",
        "A *scalar* contains a single value and has no axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lt9OfhpC6lX"
      },
      "source": [
        "import torch\n",
        "\n",
        "rank_0_tensor = torch.tensor(4)\n",
        "print(rank_0_tensor)\n",
        "print(rank_0_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgdBXldaDuRz"
      },
      "source": [
        "## Rank-1 or Vector\n",
        "A *vector* tensor is a list of values and has only one axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdizfWduDy8x"
      },
      "source": [
        "import torch\n",
        "\n",
        "rank_1_tensor = torch.tensor([1, 2, 3, 4, 5, ])\n",
        "print(rank_1_tensor)\n",
        "print(rank_1_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_lQYUuPEPzU"
      },
      "source": [
        "## Rank-2 or Matrix\n",
        "A *matrix* or *rank-2* tensor has two axes like a 2 dimesional arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lnPfCcHERBC"
      },
      "source": [
        "import torch\n",
        "\n",
        "rank_2_tensor = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
        "print(rank_2_tensor)\n",
        "print(rank_2_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYq4_iCnEXoS"
      },
      "source": [
        "## Rank-3 or 3 Dimensionals\n",
        "Tensor with 3 axes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N9zx88tEcfH"
      },
      "source": [
        "import torch\n",
        "\n",
        "rank_3_tensor = torch.tensor(\n",
        "    [[[0, 1, 2, 3, ],\n",
        "      [4, 5, 6, 7, ]],\n",
        "     [[8, 9, 10, 11, ],\n",
        "      [12, 13, 14, 15, ]],\n",
        "     [[16, 17, 18, 19, ],\n",
        "      [20, 21, 22, 23]], ])\n",
        "print(rank_3_tensor)\n",
        "print(rank_3_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufqjMttME95n"
      },
      "source": [
        "![3Dimensions-1](https://user-images.githubusercontent.com/85147048/120790992-d15b8b00-c55d-11eb-9487-6ce3cb3ca0b3.jpg)\n",
        "It is easier to construct the multidimensional tension with the last element of the shape/size. In this example (tensor size [3, 2, 4]), you start with 4 elements on an axis, 2 on another axis becoming 4 by 2 tension, and 3 on the last axis becoming 4 by 2 by 3 tension. In addition, it's easier to keep track of your multidimensional tensions when you keep the same format consistently.\n",
        "For example, construct starting on the x-axis, y-axis, z-axis, then x-axis again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd2k20WtFOLC"
      },
      "source": [
        "## Rank-4 tensor, and higher.\n",
        "Basically a stack of the matrix tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLD5b7UoFRCp"
      },
      "source": [
        "import torch\n",
        "\n",
        "rank_4_tensor = torch.zeros([3, 2, 2, 3, ])\n",
        "print(rank_4_tensor)\n",
        "print(rank_4_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkdh0vC6FUt3"
      },
      "source": [
        "![4+Dimensions](https://user-images.githubusercontent.com/85147048/120795255-73ca3d00-c563-11eb-8ba9-19736313a134.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7v6RwbFGEsD"
      },
      "source": [
        "# Tensor's Operations\n",
        "There are over a hundred tensor oparetions, including arithmetic, linear algebra, matrix manipulation, sampling, and more [here](https://pytorch.org/docs/stable/torch.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eqGXHENGJ4P"
      },
      "source": [
        "## Tensor on CUDA/CPU\n",
        "Since we have talked about CUDA in the installation section, we can move our tensor to GPU if available. By default, tensors are created on the CPU. However, you can move run them on GPU at a higher speed than on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VboDwfCGUzQ"
      },
      "source": [
        "import torch\n",
        "\n",
        "tensor_cpu = torch.rand([2, 2, 2, ])\n",
        "\n",
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    tensor_cuda = tensor_cpu.to('cuda')\n",
        "    # .to move the tensor to your gpu\n",
        "    print(tensor_cuda)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w1QouVMHC_I"
      },
      "source": [
        "*note:* device='cuda:0' is your GPU index at 0. Useful when you have multiple GPUs. <br/>\n",
        "*note:* If you don't see any output. Make sure to enable cuda by <br/>\n",
        "\n",
        "``` Go to Menu > Runtime > Change runtime type. ``` <br/>\n",
        "\n",
        "Change hardware acceleration to GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yegbHO7EGQmD"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Set the cuda0 to be the first GPU (index 0)\n",
        "    cuda0 = torch.device(\"cuda\")\n",
        "    \n",
        "    # cuda1 = torch.device(\"cuda:1) # second and more GPUs if available\n",
        "    # Cross-GPU operations are not allowed by default.\n",
        "    \n",
        "    x = torch.ones(3, device=cuda0)\n",
        "    y = torch.ones(3)\n",
        "    y = y.to(cuda0) # Move tensor y to GPU\n",
        "    \n",
        "    # This will be performed on the GPU \n",
        "    z = x + y\n",
        "    \n",
        "    # z.numpy() will not work as it can handle only CPU tensor \n",
        "    # Would have to move it back to CPU if you would like to convert\n",
        "    z = z.to(\"cpu\")\n",
        " \n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvC7M5tQH3z0"
      },
      "source": [
        "## Standard numpy-like indexing and slicing\n",
        "Access, print, or edit different indexes.\n",
        "A coding example from [PyTorch](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)<br/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXlpr4Z1H6DO"
      },
      "source": [
        "import torch\n",
        "\n",
        "tensor = torch.ones(4, 4)\n",
        "print('First row: ',tensor[0])\n",
        "print('First column: ', tensor[:, 0])\n",
        "print('Last column:', tensor[..., -1])\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sm45I3zJIIRX"
      },
      "source": [
        "More example codes in the included files/codes in [GitHub.](https://github.com/jadepanths/Python-PyTorch-Tutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kzgb2X1JA62"
      },
      "source": [
        "## Joining Tensors\n",
        "*torch.stack* **stacks** a sequence of tensors along a **new dimension**<br/>\n",
        "*torch.cat* con**cat**enates the sequence of tensors in the **given dimension.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg5HksEMJFG9"
      },
      "source": [
        "import torch\n",
        "\n",
        "t1 = torch.tensor([[1, 2],\n",
        "                   [3, 4]])\n",
        "\n",
        "t2 = torch.tensor([[5, 6],\n",
        "                   [7, 8]])\n",
        "\n",
        "tStack = torch.stack((t1, t2))\n",
        "print(\"stack: \\n\", tStack)\n",
        "print(\"stack dimension: \", tStack.shape)\n",
        "print()\n",
        "\n",
        "tCatDim1 = torch.cat((t1, t2), dim=0)\n",
        "print(\"cat | dim=0: \\n\", tCatDim1)\n",
        "print(\"cat | new dimension: \", tCatDim1.shape)\n",
        "print()\n",
        "\n",
        "tCatDim2 = torch.cat((t1, t2), dim=1)\n",
        "print(\"cat | dim=1: \\n\", tCatDim2)\n",
        "print(\"cat | new dimension: \", tCatDim2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdKSIMwfJLcS"
      },
      "source": [
        "So if **A** and **B** are of shape (4, 5), torch.cat((A, B), dim=0) will be of shape (8, 5), and torch.stack((A, B), dim=0) will be of shape (2, 4, 5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHMBEfUuNr53"
      },
      "source": [
        "## Arithmetic operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_RPGxrTNs7J"
      },
      "source": [
        "### Dot Multiplication\n",
        "```torch.mul(a, b)``` is a multiplication of the corresponding bits of matrix a and b. The dimensions of the two metrix are generally equal (ex: the number of elements have to match) The output metrix will keep its shape/dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c0SsrcON0Y9"
      },
      "source": [
        "import torch\n",
        "\n",
        "# dot multiplication\n",
        "t1 = torch.randn(1, 2, )\n",
        "t2 = torch.randn(1, 2, )\n",
        "\n",
        "tMul = torch.mul(t1, t2)\n",
        "\n",
        "print(\"t1: \\n\", t1)\n",
        "print(\"t2: \\n\", t2, \"\\n\")\n",
        "\n",
        "print(\"dot multiplication: \\n\", tMul, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyspiq90OGcM"
      },
      "source": [
        "### Matrix Multiplication\n",
        "```torch.mm(a, b)``` multiplies the matrix a and b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRMFFGVgOJnt"
      },
      "source": [
        "import torch\n",
        "\n",
        "print(\"\\n Matrix Multiplication \\n\")\n",
        "t1 = torch.tensor([[1, 2, 3, 4, ],\n",
        "                   [1, 2, 3, 4, ],\n",
        "                   [1, 2, 3, 4, ]])\n",
        "\n",
        "print(\"t1: \\n\", t1, \"\\n\", t1.shape, \"\\n\")\n",
        "\n",
        "t2 = torch.tensor([[1, 2],\n",
        "                   [1, 2],\n",
        "                   [1, 2],\n",
        "                   [1, 2]])\n",
        "\n",
        "print(\"t2: \\n\", t2, \"\\n\", t2.shape, \"\\n\")\n",
        "\n",
        "tMM = torch.mm(t1, t2)\n",
        "print(\"matrix multiplication: \\n\", tMM, \"\\n\"\n",
        "      , tMM.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRGINNncOePW"
      },
      "source": [
        "```torch.matmul(a, b)``` A high-dimensional matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Xxv6GDwOfvD"
      },
      "source": [
        "import torch\n",
        "\n",
        "# torch.matmul(a, b)\n",
        "t1 = torch.ones(2, 4, 2)\n",
        "t2 = torch.ones(2, 2, 3)\n",
        "tMatmul = torch.matmul(t1, t2)\n",
        "print(\"matrix multiplication: \\n\", tMatmul, \"\\n\"\n",
        "      , tMatmul.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha0KOhrGPIMq"
      },
      "source": [
        "There are many more operations such as: <br/>\n",
        "\n",
        "*   ```tensor.sum()``` to sum all the elements into a single tensor value.\n",
        "*   ```tensor.item()``` to change the tensor value into Python numerical value like float.\n",
        "*   ```tensor.add_(x)``` to add all the elements with **x**.\n",
        "\n",
        "note: \" **_** \" suffix is called **In-Place operations**. Operations that store the result into the operand are called in-place. Basically you are chaning/altering the variable. For example x.copy_(y) or x.t_() will change the x."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhwLfsFskqmN"
      },
      "source": [
        "# Tensor Memory Location\n",
        "This is where you have to be careful when comverting and modifying tensors.\n",
        "As they often point to the same memory address. Like a C++ pointer, when you modify one variable, another variable will be modified as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftifTZhGktrn"
      },
      "source": [
        "import torch\n",
        "\n",
        "a = torch.ones(3)\n",
        "print(a)\n",
        "b = a.numpy()\n",
        "print(b)\n",
        "\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ithvim2Mk0gO"
      },
      "source": [
        "You can see that when modify **a** with .add_(1), **b** will be modified as well.\n",
        "The reason is that **a** and **b** both point to the same memory address. The same goes to this following example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfo1U4Xzmxkc"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "a = np.ones(3)\n",
        "print(a)\n",
        "b = torch.from_numpy(a)\n",
        "print(b)\n",
        "\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
