{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Build The Neural Network.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPz0QHRnuyNM80hasw1lr0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadepanths/Python-PyTorch-Tutorial/blob/main/Build_The_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFO5mydrz7e1"
      },
      "source": [
        "# Build The Neural Network\n",
        "The Neural networks comprise of layer or modules that perform operations on data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to build your own neural network. All the modules in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A neural network is modile itself that consists of other modules/laters. This nested structure allows for building and managing complex architectures easily.<br/>\n",
        "<br/>\n",
        "We will build a neural network to classify images in the FasionMNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWmuPQ9Tz4uZ"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jSMtV4H0DA6"
      },
      "source": [
        "# Get Device for training\n",
        "Training a model on a hardware accelerator like a GPU is faster than traing on a cpu. Therefore, we should check if cuda is available.\n",
        "\n",
        "__Note:__ To enable cuda, go to \"Runtime\" -> \"change runtime type\" -> change hardware accelerator to \"GPU\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KntW_9Ru0En-",
        "outputId": "1a3496f4-8593-4371-a56a-e2f5d8071736"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loUv67Xx0tY2"
      },
      "source": [
        "# Define the Class\n",
        "We define our neural network bu subclassing _nn.Module_, and initialize the neural network laters in ```__init__```. Every _nn.Module_ subclass implements the operations on input data in the _forward_ method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfax5lG_0v8Q"
      },
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QPanaE00AC"
      },
      "source": [
        "After creating an instance if _NeuralNetwork_, we can move it to the _device_ and print its structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX1UEuas02om",
        "outputId": "d6935373-f2af-4e16-ab20-963e35d0a4c4"
      },
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42QgRIu707Kd"
      },
      "source": [
        "Note: **ReLu** is like Riemann sums. You can approximate any continuos functions with lots of little reactangles. ReLu activations can produced lots of little rectangles. ReLu can make complicated shapes and approximate maby complicated domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4mct9ue1b7j"
      },
      "source": [
        "To use the model, we pass it the input data. This executes the model’s _forward_, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call model.forward() directly. <br/>\n",
        "<br/>\n",
        "Calling the model on the input returns a 10-dimensional tensor with a raw predicted values for each class. We get the prediction probabilities by passign ti through an instance of the ```nn.Softmax``` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txhSCjjb1ueK",
        "outputId": "e068c299-3b18-498f-e83b-1f349c7a6fa0"
      },
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class: tensor([4], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAduRjFV2HsG"
      },
      "source": [
        "# Model Layers\n",
        "Break down the layers in the FashionMNIST model. We will take a sample minibatch of 3 images of size 28x28."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_vFiPg22Iwe",
        "outputId": "58d24c45-2f2d-470f-c348-c0bb49f4301a"
      },
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT_8OIop2MFD"
      },
      "source": [
        "## Flatten\n",
        "We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert each 2D 28*28 image into a contiguous of 784 pixel values (the minibatch dimension (at dim=0) is maintained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcNpgNXt2QOF",
        "outputId": "99260ccc-9157-4ea0-e9aa-f26ff489af15"
      },
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH-nXbGQ2SaH"
      },
      "source": [
        "## Linear\n",
        "The [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RMToZ3W2XZD",
        "outputId": "bb65eedd-7ba2-4528-d67d-02904220fc8a"
      },
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gszx6zb-2xvS"
      },
      "source": [
        "## ReLU\n",
        "[nn.ReLu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) is used between the linear layers. (There is other activations to introduce non-linearity in your model.\n",
        "\n",
        "Non-linear activations create the complex mapping between the model's input and outputs. They are apllied after the linear transformation to introduce _nonlinearity_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw2POqtJ23pH",
        "outputId": "94e4a63c-dc0b-4bc4-fed1-1ebdcbb57cfa"
      },
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before ReLU: tensor([[ 0.4911, -0.4534,  0.6115,  0.0976, -0.1669,  0.2323, -0.6934,  0.5175,\n",
            "         -0.3112,  0.1438,  0.5929,  0.1103, -0.2898, -0.3941, -0.9390,  0.5804,\n",
            "          0.0720, -0.6585, -0.2521, -0.0539],\n",
            "        [ 0.4652,  0.0218,  0.7294, -0.1141,  0.0237, -0.0348, -0.5429,  0.8661,\n",
            "         -0.1934,  0.0566,  0.4220, -0.1997, -0.3871,  0.1104, -0.9119,  0.5691,\n",
            "          0.4418, -0.3832, -0.5984, -0.1338],\n",
            "        [ 0.1519, -0.3420,  0.7074,  0.0182, -0.2008, -0.0081, -0.5623,  0.7078,\n",
            "         -0.0886, -0.1825,  0.2294,  0.5713, -0.1302, -0.3287, -0.7522,  0.3468,\n",
            "          0.1062, -0.5794, -0.0224, -0.0202]], grad_fn=<AddmmBackward>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.4911, 0.0000, 0.6115, 0.0976, 0.0000, 0.2323, 0.0000, 0.5175, 0.0000,\n",
            "         0.1438, 0.5929, 0.1103, 0.0000, 0.0000, 0.0000, 0.5804, 0.0720, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.4652, 0.0218, 0.7294, 0.0000, 0.0237, 0.0000, 0.0000, 0.8661, 0.0000,\n",
            "         0.0566, 0.4220, 0.0000, 0.0000, 0.1104, 0.0000, 0.5691, 0.4418, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1519, 0.0000, 0.7074, 0.0182, 0.0000, 0.0000, 0.0000, 0.7078, 0.0000,\n",
            "         0.0000, 0.2294, 0.5713, 0.0000, 0.0000, 0.0000, 0.3468, 0.1062, 0.0000,\n",
            "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWSEnPsc27Zz"
      },
      "source": [
        "## Sequential\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. This data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quicl network like *seq_modules*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGSetwij28ba"
      },
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPWxc6gi09LG"
      },
      "source": [
        "## Softmax\n",
        "The last linear layer of the neural network returns *logits* - raw values in [infty, infty] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to value [0, 1] representing the model's predicted probabilities for each class. ```dim``` parameter indicates the dimension along which the values must sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O00l5hTw14g-",
        "outputId": "b4340f58-6056-4d17-d2ed-9505e3fc4436"
      },
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "print(pred_probab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0967, 0.0908, 0.1688, 0.0947, 0.0887, 0.0989, 0.0850, 0.0854, 0.0745,\n",
            "         0.1165],\n",
            "        [0.1063, 0.0874, 0.1402, 0.0855, 0.0968, 0.0981, 0.0856, 0.0956, 0.0931,\n",
            "         0.1115],\n",
            "        [0.0880, 0.0937, 0.1490, 0.0981, 0.0933, 0.1009, 0.0861, 0.0969, 0.0801,\n",
            "         0.1140]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwZl_oAW2eXp"
      },
      "source": [
        "## Model Parameters\n",
        "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training. Subclassing ```nn.Module``` automatically tracks all fields defined inside your model object, and makes all parameters accessible using your model’s ```parameters()``` or ```named_parameters()``` methods.\n",
        "\n",
        "In this example, we iterate over each parameter, and print its size and a preview of its values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s7xeJAT2paU",
        "outputId": "575be75d-8da1-46da-ae0a-c365a61ec9e7"
      },
      "source": [
        "print(\"Model structure: \", model, \"\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model structure:  NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ") \n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 8.4528e-03, -2.6379e-02,  4.6932e-03,  ...,  5.4866e-05,\n",
            "          9.7483e-03,  1.1605e-02],\n",
            "        [-2.9542e-02, -2.6049e-03,  2.8827e-02,  ...,  1.4241e-03,\n",
            "          3.4997e-02, -1.0284e-02]], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0343, 0.0138], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0050, -0.0251, -0.0393,  ...,  0.0376,  0.0372,  0.0147],\n",
            "        [-0.0248,  0.0293, -0.0193,  ..., -0.0009,  0.0169,  0.0128]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0024, 0.0401], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0264,  0.0291,  0.0384,  ...,  0.0342, -0.0369,  0.0264],\n",
            "        [-0.0118, -0.0401,  0.0366,  ...,  0.0135,  0.0339,  0.0260]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0324, -0.0066], device='cuda:0', grad_fn=<SliceBackward>) \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}