{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transforms.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMao5le1/+Le8wDkyvnoqT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadepanths/Python-PyTorch-Tutorial/blob/main/Transforms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0-5oX6Qx62n"
      },
      "source": [
        "# Transforms\n",
        "Data does not always come in its final processed form and is required for traning machine learning algorithms. We use **transforms** to preform some manupulation of the data and make it suitable for traning like a raw ingredient where we need to cook it.\n",
        "<br/>\n",
        "<br/>\n",
        "All TorchVision datasets have two parameters - transform_ to modify the features and _target_transform_ to modify the labels - that accept callables containing the transformation logic. There are many commnon transforms here, [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html).\n",
        "<br/>\n",
        "<br/>\n",
        "Using the example from pytorch.org (FashionMNIST), the FashionMNIST features are in PIL image format, and the labels are intergers. For training, we need the feature as normalized tensors, and the labels as one-hot encoded tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fD-LNeDxxQw"
      },
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNZsjuwEyHNy"
      },
      "source": [
        "# ToTensor() \n",
        "Converts a PIL image or NumPy _ndarray_ into a _FloatTensor_ and scale the image's pixel intensity values in the range [0.0 to 1.0]. The works for the image with elements that are in range from 0 to 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVhTKLLAyJEL"
      },
      "source": [
        "# Lambda Transforms\n",
        "apply any user-defined lambda function. Here, we difine a function to turn integer into a one-hot encoded tensor. It first creates a zero tensor of size 10 (the number of labels in our dataset) and called [scatter_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.scatter_) which assigns a value=1 on the index as given by the label y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmZaxLOdyMeh"
      },
      "source": [
        "target_transform = Lambda(lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWCqL5QiySr2"
      },
      "source": [
        "# One-hot Encoded Tensor\n",
        "Since we have mentioned one-hot encoded tensor several times, we are going to take a look what it actually is. [Here](https://datascience.stackexchange.com/questions/30215/what-is-one-hot-encoding-in-tensorflow) is a explaination from Djib2011 on a Stack.Exchange question.\n",
        "<br/>\n",
        "Suppose we have a catagorical feature in our dataset like colour. Your samples can only be either red, yellow, or blue. In machine learning algorith, we have to pass the argument as a number instead of strings. So we mapped the colours like this: <br/>\n",
        "<br/>\n",
        "red --> 1 <br/>\n",
        "yellow --> 2 <br/>\n",
        "blue --> 3 <br/>\n",
        "<br/>\n",
        "We have replaced the string with the mapped value. However, this method can create negative side effects in our model when dealing with numbers. For example, blue is larger than yellow because 3 is larger than 2. Or red and yellow combied is equal to blue because of 1 + 2 = 3. The model has no way of knowing that these data was catagorical and then were mapped as intergers.<br/> <br/>\n",
        "Now is where **one-hot encoding** comes in handy. We create *N* **new features**. where *N* is the number of unique values in the orignal feature, where _N_ is the number of unique values in the original feature. In our example, _N_ would be eqaul to 3 as we only have 3 unique colours: red, yellow, and blue. <br/>\n",
        "<br/>\n",
        "Each of these features is binary and would correspond to **one** of these unique values. In our example, the first feature would be a binary feature tellinus if that samle is red or not. The second would be the same this for yellow, and the Third for blue. <br/>\n",
        "<br/>\n",
        "An example of such a transformation is illustrated below: <br/>\n",
        "![mtimFxh](https://user-images.githubusercontent.com/85147048/121554816-a9c46100-ca3c-11eb-9b19-9bfefe159680.png)<br/>\n",
        "Note, that because this approach increases the dimensionality of the dataset, if we have a feature that takes many unique values, we may want to use a more sparse encoding."
      ]
    }
  ]
}